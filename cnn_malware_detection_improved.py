'''
Improved Deep Eigenspace Learning CNN for IoT Malware Detection
Addresses overfitting issues identified in the analysis:
- Uses cleaned dataset (no duplicates)
- Implements 10-fold cross-validation like the paper
- Applies class weights for imbalanced data
- Enhanced regularization and early stopping
- Multiple evaluation metrics (not just accuracy)
'''

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
import numpy as np
import pickle
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
import seaborn as sns
import os
from collections import defaultdict

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Create results directory
RESULTS_DIR = "CNN Testing Results Improved"
os.makedirs(RESULTS_DIR, exist_ok=True)
print(f"Results will be saved to: {RESULTS_DIR}/")

class EigenspaceDataset(Dataset):
    """Dataset for eigenspace embeddings"""
    def __init__(self, embeddings, labels):
        self.embeddings = torch.FloatTensor(embeddings)
        self.labels = torch.LongTensor(labels)
    
    def __len__(self):
        return len(self.embeddings)
    
    def __getitem__(self, idx):
        return self.embeddings[idx], self.labels[idx]

class ImprovedDeepEigenspaceCNN(nn.Module):
    """
    Improved CNN with reduced complexity and enhanced regularization
    """
    def __init__(self, input_dim=984, num_classes=2, dropout_rate=0.6):
        super(ImprovedDeepEigenspaceCNN, self).__init__()
        
        # Reshape 984D to 2D (12 eigenvectors × 82 features)
        self.reshape_dim = (12, 82)
        
        # Reduced complexity: fewer filters and layers
        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1)
        
        # Batch normalization
        self.bn1 = nn.BatchNorm2d(16)
        self.bn2 = nn.BatchNorm2d(32)
        
        # Pooling
        self.pool = nn.MaxPool2d(2, 2)
        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))
        
        # Reduced fully connected layers
        self.fc1 = nn.Linear(32 * 4 * 4, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, num_classes)
        
        # Enhanced dropout
        self.dropout = nn.Dropout(dropout_rate)
        
    def forward(self, x):
        batch_size = x.size(0)
        x = x.view(batch_size, 1, self.reshape_dim[0], self.reshape_dim[1])
        
        # Convolutional layers with dropout
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.dropout(x)
        x = self.pool(x)
        
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.dropout(x)
        x = self.adaptive_pool(x)
        
        # Flatten and fully connected layers
        x = x.view(batch_size, -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        
        return x

def train_model_with_early_stopping(model, train_loader, val_loader, class_weights, 
                                   num_epochs=100, patience=15):
    """Train model with early stopping and class weights"""
    
    # Use class weights in loss function
    criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))
    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-3)  # Reduced learning rate
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)
    
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    
    best_val_loss = float('inf')
    best_model_state = None
    patience_counter = 0
    
    model.to(device)
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for embeddings, labels in train_loader:
            embeddings, labels = embeddings.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(embeddings)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for embeddings, labels in val_loader:
                embeddings, labels = embeddings.to(device), labels.to(device)
                outputs = model(embeddings)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
        
        # Calculate metrics
        train_acc = 100 * train_correct / train_total
        val_acc = 100 * val_correct / val_total
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)
        
        # Early stopping logic
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_state = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1
        
        scheduler.step(avg_val_loss)
        
        if epoch % 10 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}]')
            print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')
            print(f'LR: {optimizer.param_groups[0]["lr"]:.6f}')
        
        # Early stopping
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break
    
    # Load best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    return model, train_losses, val_losses, train_accuracies, val_accuracies

def evaluate_model_comprehensive(model, test_loader, fold_num=None):
    """Comprehensive model evaluation with multiple metrics"""
    model.eval()
    all_predictions = []
    all_labels = []
    all_probabilities = []
    
    with torch.no_grad():
        for embeddings, labels in test_loader:
            embeddings, labels = embeddings.to(device), labels.to(device)
            outputs = model(embeddings)
            probabilities = F.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probabilities.extend(probabilities[:, 1].cpu().numpy())
    
    # Calculate comprehensive metrics
    accuracy = 100 * sum(p == l for p, l in zip(all_predictions, all_labels)) / len(all_labels)
    
    # Handle edge case where only one class in test set
    if len(set(all_labels)) == 1:
        auc_score = 0.5  # Random performance
        f1 = 0.0
    else:
        try:
            auc_score = roc_auc_score(all_labels, all_probabilities)
            f1 = f1_score(all_labels, all_predictions, average='weighted')
        except:
            auc_score = 0.5
            f1 = 0.0
    
    # Print detailed results for this fold
    if fold_num is not None:
        print(f"\n--- Fold {fold_num} Results ---")
        print(f"Accuracy: {accuracy:.2f}%")
        print(f"AUC: {auc_score:.4f}")
        print(f"F1-Score: {f1:.4f}")
        
        if len(set(all_labels)) > 1:
            print("Classification Report:")
            print(classification_report(all_labels, all_predictions, 
                                      target_names=['Benign', 'Malware'], 
                                      zero_division=0))
    
    return accuracy, auc_score, f1, all_predictions, all_labels, all_probabilities

def cross_validate_cnn(X, y, n_folds=10):
    """Perform stratified k-fold cross-validation"""
    print(f"\n=== {n_folds}-Fold Cross-Validation ===")
    
    # Calculate class weights
    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
    print(f"Class weights: {class_weights}")
    
    # Stratified K-Fold
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
    
    # Results storage
    fold_results = defaultdict(list)
    all_predictions = []
    all_true_labels = []
    
    for fold, (train_val_idx, test_idx) in enumerate(skf.split(X, y)):
        print(f"\n=== Training Fold {fold + 1}/{n_folds} ===")
        
        # Split train_val into train and validation (80:20)
        train_val_size = len(train_val_idx)
        val_size = train_val_size // 5
        train_idx = train_val_idx[:-val_size]
        val_idx = train_val_idx[-val_size:]
        
        # Create data subsets
        X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]
        y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]
        
        print(f"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
        print(f"Train classes: {np.bincount(y_train)}")
        print(f"Test classes: {np.bincount(y_test)}")
        
        # Create datasets and loaders
        train_dataset = EigenspaceDataset(X_train, y_train)
        val_dataset = EigenspaceDataset(X_val, y_val)
        test_dataset = EigenspaceDataset(X_test, y_test)
        
        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
        
        # Create and train model
        model = ImprovedDeepEigenspaceCNN()
        model, _, _, _, _ = train_model_with_early_stopping(
            model, train_loader, val_loader, class_weights, num_epochs=50, patience=10
        )
        
        # Evaluate
        accuracy, auc, f1, predictions, true_labels, probabilities = evaluate_model_comprehensive(
            model, test_loader, fold + 1
        )
        
        # Store results
        fold_results['accuracy'].append(accuracy)
        fold_results['auc'].append(auc)
        fold_results['f1'].append(f1)
        
        all_predictions.extend(predictions)
        all_true_labels.extend(true_labels)
    
    # Calculate overall statistics
    print(f"\n=== Overall {n_folds}-Fold CV Results ===")
    for metric in ['accuracy', 'auc', 'f1']:
        values = fold_results[metric]
        mean_val = np.mean(values)
        std_val = np.std(values)
        print(f"{metric.upper()}: {mean_val:.4f} ± {std_val:.4f}")
        
        # Check for overfitting indicators
        if metric == 'accuracy' and std_val < 1.0:
            print(f"  ⚠️  Low variance in {metric} - possible overfitting!")
        if metric == 'accuracy' and mean_val > 98.0:
            print(f"  ⚠️  Very high {metric} - possible overfitting!")
    
    return fold_results

def plot_cv_results(fold_results, save_path):
    """Plot cross-validation results"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    metrics = ['accuracy', 'auc', 'f1']
    titles = ['Accuracy (%)', 'AUC Score', 'F1 Score']
    
    for i, (metric, title) in enumerate(zip(metrics, titles)):
        values = fold_results[metric]
        axes[i].boxplot([values], labels=['10-Fold CV'])
        axes[i].scatter([1] * len(values), values, alpha=0.6, s=50)
        axes[i].set_title(f'{title}\nMean: {np.mean(values):.3f} ± {np.std(values):.3f}')
        axes[i].grid(True, alpha=0.3)
        
        # Add individual fold values as text
        for j, val in enumerate(values):
            axes[i].text(1.1, val, f'F{j+1}', fontsize=8, alpha=0.7)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Cross-validation results plot saved to: {save_path}")

def main():
    print("=== Improved CNN with Overfitting Prevention ===")
    
    # Load cleaned data
    print("Loading cleaned dataset...")
    try:
        with open("Overfitting Analysis Results/cleaned_dataset.pkl", "rb") as f:
            cleaned_data = pickle.load(f)
            X = np.array(cleaned_data['embeddings'])
            y = np.array(cleaned_data['labels'])
        print(f"✅ Using cleaned dataset: {len(X)} samples, {X.shape[1]} features")
    except FileNotFoundError:
        print("⚠️  Cleaned dataset not found, using original data")
        with open("X_graph_embeddings.pkl", "rb") as f:
            X = pickle.load(f)
        with open("improved_cig_output.pkl", "rb") as f:
            data = pickle.load(f)
            y = np.array(data["labels"])
    
    print(f"Class distribution: Benign={np.sum(y==0)}, Malware={np.sum(y==1)}")
    print(f"Class imbalance ratio: {np.sum(y==0) / np.sum(y==1):.1f}:1")
    
    # Normalize features
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    
    # Perform 10-fold cross-validation
    fold_results = cross_validate_cnn(X, y, n_folds=10)
    
    # Plot and save results
    plot_path = os.path.join(RESULTS_DIR, 'improved_cnn_cv_results.png')
    plot_cv_results(fold_results, plot_path)
    
    # Save detailed results
    results_path = os.path.join(RESULTS_DIR, 'improved_cnn_results.pkl')
    with open(results_path, 'wb') as f:
        pickle.dump(fold_results, f)
    
    print(f"\n=== Final Summary ===")
    print(f"Results saved to: {RESULTS_DIR}/")
    print(f"Key improvements implemented:")
    print(f"✅ Used cleaned dataset (removed duplicates)")
    print(f"✅ Applied 10-fold stratified cross-validation")
    print(f"✅ Used class weights for imbalanced data")
    print(f"✅ Reduced model complexity")
    print(f"✅ Enhanced regularization (dropout, weight decay)")
    print(f"✅ Early stopping to prevent overfitting")
    print(f"✅ Multiple evaluation metrics (accuracy, AUC, F1)")

if __name__ == "__main__":
    main()
