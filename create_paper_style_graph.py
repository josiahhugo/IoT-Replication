'''
Create Paper-Style Graph for Junk Code Resilience Results
Matches the visualization style from the original paper
Saves all outputs in organized folders
'''
import matplotlib
matplotlib.use('Agg')  # Set non-interactive backend
import matplotlib.pyplot as plt
import numpy as np
import json
import pickle
import os
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedShuffleSplit

def create_output_folders():
    """
    Create organized folder structure for outputs
    """
    folders = [
        'Junk Code Results/Graphs',
        'Junk Code Results/Data',
        'Junk Code Results/Analysis'
    ]
    
    for folder in folders:
        os.makedirs(folder, exist_ok=True)
        print(f"ğŸ“ Created folder: {folder}")
    
    return folders

def algorithm2_junk_insertion(adjacency_matrix, junk_percentage_k):
    """Same implementation as before"""
    W = adjacency_matrix.copy()
    total_elements = W.size
    num_to_select = max(1, int(total_elements * junk_percentage_k / 100))
    selected_indices = np.random.randint(0, total_elements, size=num_to_select)
    
    for idx in selected_indices:
        row, col = np.unravel_index(idx, W.shape)
        W[row, col] += 1
    
    # Normalize
    row_sums = W.sum(axis=1)
    row_sums[row_sums == 0] = 1
    W_normalized = W / row_sums[:, np.newaxis]
    
    # Extract features
    eigenvalues, eigenvectors = np.linalg.eigh(W_normalized)
    e1 = eigenvectors[:, 0] if eigenvectors.shape[1] > 0 else np.zeros(W.shape[0])
    e2 = eigenvectors[:, 1] if eigenvectors.shape[1] > 1 else np.zeros(W.shape[0])
    l1 = eigenvalues[0] if len(eigenvalues) > 0 else 0
    l2 = eigenvalues[1] if len(eigenvalues) > 1 else 0
    
    modified_features = np.concatenate([e1, e2, [l1, l2]])
    return modified_features, W_normalized

def collect_detailed_metrics():
    """
    Collect detailed metrics (accuracy, precision, recall, F-measure) for each junk percentage
    """
    print("=== Collecting Detailed Metrics for Paper-Style Graph ===")
    
    # Load data
    with open("improved_cig_output.pkl", "rb") as f:
        data = pickle.load(f)
    
    with open("X_graph_embeddings.pkl", "rb") as f:
        X_embeddings = np.array(pickle.load(f))
    
    labels = data["labels"]
    
    # Create test setup matching our previous experiments
    sss = StratifiedShuffleSplit(n_splits=1, test_size=100, random_state=42)
    indices = list(range(len(labels)))
    
    for train_indices, test_indices in sss.split(indices, labels):
        embedding_dim = int(np.sqrt(X_embeddings.shape[1]))
        
        # Prepare training data
        train_matrices = []
        train_labels_subset = []
        for i in train_indices:
            matrix = X_embeddings[i][:embedding_dim*embedding_dim].reshape(embedding_dim, embedding_dim)
            matrix = (matrix + matrix.T) / 2
            train_matrices.append(matrix)
            train_labels_subset.append(labels[i])
        
        # Prepare test data
        test_matrices = []
        test_labels_subset = []
        for i in test_indices:
            matrix = X_embeddings[i][:embedding_dim*embedding_dim].reshape(embedding_dim, embedding_dim)
            matrix = (matrix + matrix.T) / 2
            test_matrices.append(matrix)
            test_labels_subset.append(labels[i])
    
    print(f"Train: {len(train_labels_subset)}, Test: {len(test_labels_subset)}")
    
    # Extract training features
    train_features = []
    for matrix in train_matrices:
        eigenvalues, eigenvectors = np.linalg.eigh(matrix)
        features = np.concatenate([
            eigenvectors[:, 0], eigenvectors[:, 1],
            [eigenvalues[0], eigenvalues[1]]
        ])
        train_features.append(features)
    
    train_features = np.array(train_features)
    
    # Train classifier
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(train_features, train_labels_subset)
    
    # Test different junk percentages - using same as paper
    junk_percentages = [5, 10, 15, 20, 25, 30]  # Remove 0% to match paper
    metrics_data = {
        'junk_percentages': junk_percentages,
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f_measure': []
    }
    
    # First get baseline (0% junk)
    test_features_baseline = []
    for matrix in test_matrices:
        eigenvalues, eigenvectors = np.linalg.eigh(matrix)
        features = np.concatenate([
            eigenvectors[:, 0], eigenvectors[:, 1],
            [eigenvalues[0], eigenvalues[1]]
        ])
        test_features_baseline.append(features)
    test_features_baseline = np.array(test_features_baseline)
    baseline_accuracy = rf.score(test_features_baseline, test_labels_subset) * 100
    
    print(f"Baseline (0% junk): {baseline_accuracy:.1f}%")
    
    for junk_pct in junk_percentages:
        print(f"Testing {junk_pct}% junk insertion...")
        
        # Apply junk insertion
        test_features = []
        for matrix in test_matrices:
            modified_features, _ = algorithm2_junk_insertion(matrix, junk_pct)
            test_features.append(modified_features)
        test_features = np.array(test_features)
        
        # Ensure feature compatibility
        min_features = min(train_features.shape[1], test_features.shape[1])
        test_features_compat = test_features[:, :min_features]
        
        if test_features_compat.shape[1] < train_features.shape[1]:
            padding = np.zeros((test_features_compat.shape[0], 
                              train_features.shape[1] - test_features_compat.shape[1]))
            test_features_compat = np.hstack([test_features_compat, padding])
        
        # Get predictions
        predictions = rf.predict(test_features_compat)
        
        # Calculate metrics
        accuracy = rf.score(test_features_compat, test_labels_subset) * 100
        
        # Handle case where we might have only one class in predictions
        try:
            precision = precision_score(test_labels_subset, predictions, average='weighted', zero_division=1) * 100
            recall = recall_score(test_labels_subset, predictions, average='weighted', zero_division=1) * 100
            f_measure = f1_score(test_labels_subset, predictions, average='weighted', zero_division=1) * 100
        except:
            # Fallback for edge cases
            precision = accuracy
            recall = accuracy
            f_measure = accuracy
        
        metrics_data['accuracy'].append(accuracy)
        metrics_data['precision'].append(precision)
        metrics_data['recall'].append(recall)
        metrics_data['f_measure'].append(f_measure)
        
        print(f"  Accuracy: {accuracy:.1f}%, Precision: {precision:.1f}%, Recall: {recall:.1f}%, F-measure: {f_measure:.1f}%")
    
    # Add baseline to metrics for reference
    metrics_data['baseline_accuracy'] = baseline_accuracy
    
    return metrics_data

def create_paper_style_graph(metrics_data):
    """
    Create a graph exactly matching the paper's style and format
    """
    print("\n=== Creating Paper-Style Graph (Exact Match) ===")
    
    # Set up the plot with exact paper styling
    plt.figure(figsize=(8, 6))
    plt.rcParams.update({
        'font.size': 11,
        'font.family': 'sans-serif',
        'axes.linewidth': 1.0,
        'grid.alpha': 0.3,
        'axes.edgecolor': 'black',
        'axes.facecolor': 'white'
    })
    
    # Plot data with exact markers and colors from the paper
    junk_percentages = metrics_data['junk_percentages']
    
    # Blue line with square markers for Accuracy
    plt.plot(junk_percentages, metrics_data['accuracy'], 
             marker='s', markersize=7, linewidth=2, 
             label='Accuracy', color='#0066CC', 
             markerfacecolor='white', markeredgewidth=1.5, markeredgecolor='#0066CC')
    
    # Orange line with triangle markers for Precision  
    plt.plot(junk_percentages, metrics_data['precision'], 
             marker='^', markersize=7, linewidth=2, 
             label='Precision', color='#FF8C00', 
             markerfacecolor='white', markeredgewidth=1.5, markeredgecolor='#FF8C00')
    
    # Green line with circle markers for Recall
    plt.plot(junk_percentages, metrics_data['recall'], 
             marker='o', markersize=7, linewidth=2, 
             label='Recall', color='#228B22', 
             markerfacecolor='white', markeredgewidth=1.5, markeredgecolor='#228B22')
    
    # Red line with diamond markers for F-Measure
    plt.plot(junk_percentages, metrics_data['f_measure'], 
             marker='D', markersize=6, linewidth=2, 
             label='F-Measure', color='#DC143C', 
             markerfacecolor='white', markeredgewidth=1.5, markeredgecolor='#DC143C')
    
    # Exact styling to match the paper
    plt.xlabel('Junk OpCode Injection Percentage', fontsize=12)
    plt.ylabel('Metric Value (%)', fontsize=12)
    
    # Set axis limits exactly like the paper
    plt.xlim(0, 35)
    plt.ylim(82, 100)  # Adjusted to show our data range better
    
    # Set ticks to match paper
    plt.xticks([5, 10, 15, 20, 25, 30], [f"{x}%" for x in [5, 10, 15, 20, 25, 30]])
    plt.yticks(range(82, 101, 2))
    
    # Grid styling
    plt.grid(True, alpha=0.3, linestyle='-', linewidth=0.5, color='gray')
    
    # Legend in bottom right like the paper
    plt.legend(loc='lower left', frameon=True, fancybox=False, shadow=False, 
              framealpha=1.0, edgecolor='black', fontsize=10)
    
    # Clean layout
    plt.tight_layout()
    
    # Save the graph
    plt.savefig('Junk Code Results/Graphs/paper_exact_style_graph.png', dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    plt.savefig('Junk Code Results/Graphs/paper_exact_style_graph.pdf', bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    
    print("âœ… Paper-exact style graph saved to:")
    print("   - Junk Code Results/Graphs/paper_exact_style_graph.png")
    print("   - Junk Code Results/Graphs/paper_exact_style_graph.pdf")
    
    plt.close()  # Close to free memory

def create_comparison_graph(metrics_data):
    """
    Create the detailed analysis graph you liked
    """
    plt.figure(figsize=(12, 5))
    
    # Subplot 1: Performance degradation
    plt.subplot(1, 2, 1)
    junk_percentages = metrics_data['junk_percentages']
    baseline_accuracy = metrics_data['baseline_accuracy']
    
    # Calculate performance drops
    accuracy_drops = [(baseline_accuracy - acc) for acc in metrics_data['accuracy']]
    
    bars = plt.bar(junk_percentages, accuracy_drops, alpha=0.7, color='coral', 
                   edgecolor='darkred', linewidth=1.5)
    
    # Add value labels on bars
    for bar, drop in zip(bars, accuracy_drops):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{drop:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    plt.xlabel('Junk Code Percentage (%)', fontsize=12)
    plt.ylabel('Performance Drop (%)', fontsize=12)
    plt.title('Performance Degradation vs Junk Code\n(Lower is Better)', fontsize=13, fontweight='bold')
    plt.grid(True, alpha=0.3, axis='y')
    plt.ylim(0, max(accuracy_drops) * 1.2)
    
    # Subplot 2: Resilience scores
    plt.subplot(1, 2, 2)
    resilience_scores = [(acc / baseline_accuracy) * 100 for acc in metrics_data['accuracy']]
    
    line = plt.plot(junk_percentages, resilience_scores, 'o-', linewidth=3, markersize=10, 
                   color='green', markerfacecolor='lightgreen', markeredgecolor='darkgreen',
                   markeredgewidth=2)
    
    # Add value labels on points
    for x, y in zip(junk_percentages, resilience_scores):
        plt.text(x, y + 1, f'{y:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    plt.xlabel('Junk Code Percentage (%)', fontsize=12)
    plt.ylabel('Resilience Score (%)', fontsize=12)
    plt.title('Resilience to Junk Code Attacks\n(Higher is Better)', fontsize=13, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.ylim(80, 100)
    
    # Add resilience threshold lines
    plt.axhline(y=90, color='red', linestyle='--', alpha=0.7, linewidth=2, label='90% Excellent')
    plt.axhline(y=80, color='orange', linestyle='--', alpha=0.7, linewidth=2, label='80% Good')
    plt.legend(loc='lower right')
    
    plt.tight_layout()
    
    # Save in Results/Analysis folder
    plt.savefig('Junk Code Results/Analysis/junk_code_analysis_detailed.png', dpi=300, bbox_inches='tight')
    plt.savefig('Junk Code Results/Analysis/junk_code_analysis_detailed.pdf', bbox_inches='tight')
    
    print("âœ… Detailed analysis saved to:")
    print("   - Junk Code Results/Analysis/junk_code_analysis_detailed.png")
    print("   - Junk Code Results/Analysis/junk_code_analysis_detailed.pdf")
    
    plt.close()

def print_detailed_results(metrics_data):
    """
    Print detailed results table and save to file
    """
    print("\n=== DETAILED RESULTS TABLE ===")
    
    # Create results table
    table_content = []
    table_content.append("Junk%  | Accuracy | Precision | Recall | F-Measure | Resilience")
    table_content.append("-" * 65)
    
    baseline_accuracy = metrics_data['baseline_accuracy']
    
    # Add baseline row
    line = f"  0%  | {baseline_accuracy:7.1f}% |     N/A   |  N/A  |     N/A   |   1.000"
    table_content.append(line)
    print(line)
    
    for i, junk_pct in enumerate(metrics_data['junk_percentages']):
        acc = metrics_data['accuracy'][i]
        prec = metrics_data['precision'][i]
        rec = metrics_data['recall'][i]
        f_mes = metrics_data['f_measure'][i]
        resilience = acc / baseline_accuracy
        
        line = f"{junk_pct:4d}%  | {acc:7.1f}% | {prec:8.1f}% | {rec:5.1f}% | {f_mes:8.1f}% | {resilience:8.3f}"
        table_content.append(line)
        print(line)
    
    # Summary statistics
    avg_resilience = np.mean([metrics_data['accuracy'][i] / baseline_accuracy 
                             for i in range(len(metrics_data['accuracy']))])
    
    summary_lines = [
        f"\nSummary:",
        f"  Baseline accuracy: {baseline_accuracy:.1f}%",
        f"  Average resilience: {avg_resilience:.3f}",
        f"  Min accuracy: {min(metrics_data['accuracy']):.1f}%",
        f"  Max performance drop: {baseline_accuracy - min(metrics_data['accuracy']):.1f}%"
    ]
    
    for line in summary_lines:
        table_content.append(line)
        print(line)
    
    # Save table to file
    with open('Junk Code Results/Data/detailed_results_table.txt', 'w') as f:
        f.write('\n'.join(table_content))

    print(f"\nâœ… Results table saved to: Junk Code Results/Data/detailed_results_table.txt")

def save_experiment_metadata(metrics_data):
    """
    Save metadata about the experiment
    """
    avg_resilience = np.mean([metrics_data['accuracy'][i] / metrics_data['baseline_accuracy'] 
                             for i in range(len(metrics_data['accuracy']))])
    
    metadata = {
        "experiment_name": "IoT Malware Detection - Junk Code Resilience Test",
        "algorithm": "Algorithm 2 - Eigenspace Embedding with Junk Code Insertion",
        "date": "2025-07-01",
        "description": "Paper replication testing junk code resilience using eigenspace embeddings",
        "results_summary": {
            "baseline_accuracy": round(metrics_data['baseline_accuracy'], 1),
            "average_resilience": round(avg_resilience, 3),
            "min_accuracy": round(min(metrics_data['accuracy']), 1),
            "max_performance_drop": round(metrics_data['baseline_accuracy'] - min(metrics_data['accuracy']), 1)
        },
        "test_parameters": {
            "junk_percentages": metrics_data['junk_percentages'],
            "test_size": 100,
            "train_size": 1107,
            "random_state": 42,
            "classifier": "RandomForestClassifier(n_estimators=100)"
        },
        "files_generated": [
            "Junk Code Results/Graphs/paper_exact_style_graph.png",
            "Junk Code Results/Graphs/paper_exact_style_graph.pdf",
            "Junk Code Results/Analysis/junk_code_analysis_detailed.png", 
            "Junk Code Results/Analysis/junk_code_analysis_detailed.pdf",
            "Junk Code Results/Data/detailed_metrics_data.json",
            "Junk Code Results/Data/detailed_results_table.txt",
            "Junk Code Results/Data/experiment_metadata.json"
        ]
    }
    
    with open('Junk Code Results/Data/experiment_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)

    print("âœ… Experiment metadata saved to: Junk Code Results/Data/experiment_metadata.json")

def main():
    """
    Main function to create paper-style visualizations with organized folder structure
    """
    print("ğŸ“Š CREATING PAPER-EXACT STYLE JUNK CODE RESILIENCE GRAPHS")
    print("=" * 60)
    
    # Create organized folder structure
    create_output_folders()
    
    # Collect detailed metrics
    metrics_data = collect_detailed_metrics()
    
    # Save metrics data in Results/Data folder
    with open('Junk Code Results/Data/detailed_metrics_data.json', 'w') as f:
        json.dump(metrics_data, f, indent=2)
    print("âœ… Metrics data saved to: Junk Code Results/Data/detailed_metrics_data.json")
    
    # Create visualizations
    create_paper_style_graph(metrics_data)  # Exact paper match
    create_comparison_graph(metrics_data)   # The analysis you liked
    
    # Print and save detailed results
    print_detailed_results(metrics_data)
    
    # Save experiment metadata
    save_experiment_metadata(metrics_data)
    
    print("\n" + "=" * 60)
    print("âœ… ALL VISUALIZATIONS AND DATA SAVED!")
    print("\nğŸ“ FILES CREATED:")
    print("ğŸ“Š Main Paper Graph:")
    print("   - Junk Code Results/Graphs/paper_exact_style_graph.png")
    print("ğŸ“ˆ Detailed Analysis (the one you liked):")
    print("   - Junk Code Results/Analysis/junk_code_analysis_detailed.png")
    print("ğŸ“„ Data & Results:")
    print("   - Junk Code Results/Data/detailed_metrics_data.json")
    print("   - Junk Code Results/Data/detailed_results_table.txt")

if __name__ == "__main__":
    main()